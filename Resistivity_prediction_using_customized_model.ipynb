{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbbc7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from model.ANN import Net\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc80e96",
   "metadata": {},
   "source": [
    "### Preprocessing of experimental electrical resistivity data for customized model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd43add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental dataset used for the model training\n",
    "train = pd.read_csv('./csv/Train.csv')\n",
    "val = pd.read_csv('./csv/Val.csv')\n",
    "total = pd.concat([train, val], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcd8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intrinsic and extrinsic features\n",
    "# t: thickness of the film [nm], depo: deposition rate [nm/s], \n",
    "# r: Average atomic radius [], del_r: Atomic radius mismatch, del_EN: Electronegativity difference, \n",
    "# S: Mixing entropy [J/mol-K], H: Heat of mixing [kJ/mol], VEC: Average valence electron concentration, Th: Homologous temperature\n",
    "# Formulas for input compositional features can be found in methods section of the manuscript\n",
    "\n",
    "# When you do not have entire extrinsic features,\n",
    "# you can still predict the electrical resistivity of thin-film alloys of interest by training a customized model using these scripts.\n",
    "\n",
    "norm_list = ['t', 'r', 'del_r', 'del_EN', 'S', 'H', 'VEC'] # Set compositional input features thin-film alloys you have\n",
    "normalize = total[norm_list]\n",
    "\n",
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "normalize[:] = scaler.fit_transform(normalize[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c324c9",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2693d63c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/1000, train loss: 84.1049, val loss: 53.2564\n",
      "epoch: 2/1000, train loss: 45.5381, val loss: 41.1953\n",
      "epoch: 3/1000, train loss: 37.4956, val loss: 36.1376\n",
      "epoch: 4/1000, train loss: 32.7140, val loss: 31.2119\n",
      "epoch: 5/1000, train loss: 28.8330, val loss: 28.2852\n",
      "epoch: 6/1000, train loss: 26.6564, val loss: 26.8351\n",
      "epoch: 7/1000, train loss: 25.4166, val loss: 25.7932\n",
      "epoch: 8/1000, train loss: 24.5347, val loss: 25.0998\n",
      "epoch: 9/1000, train loss: 23.7994, val loss: 24.5316\n",
      "epoch: 10/1000, train loss: 23.1611, val loss: 24.0662\n",
      "epoch: 11/1000, train loss: 22.5917, val loss: 23.6695\n",
      "epoch: 12/1000, train loss: 22.0389, val loss: 23.3506\n",
      "epoch: 13/1000, train loss: 21.5177, val loss: 22.8935\n",
      "epoch: 14/1000, train loss: 21.0193, val loss: 21.9791\n",
      "epoch: 15/1000, train loss: 20.5554, val loss: 21.0236\n",
      "epoch: 16/1000, train loss: 20.1121, val loss: 20.3361\n",
      "epoch: 17/1000, train loss: 19.6924, val loss: 19.6809\n",
      "epoch: 18/1000, train loss: 19.2757, val loss: 19.2663\n",
      "epoch: 19/1000, train loss: 18.9091, val loss: 19.1016\n",
      "epoch: 20/1000, train loss: 18.5497, val loss: 18.8451\n",
      "epoch: 21/1000, train loss: 18.1719, val loss: 18.3283\n",
      "epoch: 22/1000, train loss: 17.8185, val loss: 17.8604\n",
      "epoch: 23/1000, train loss: 17.5264, val loss: 17.5634\n",
      "epoch: 24/1000, train loss: 17.2593, val loss: 17.2978\n",
      "epoch: 25/1000, train loss: 17.0112, val loss: 17.0569\n",
      "epoch: 26/1000, train loss: 16.7694, val loss: 16.8212\n",
      "epoch: 27/1000, train loss: 16.5318, val loss: 16.5809\n",
      "epoch: 28/1000, train loss: 16.3080, val loss: 16.3367\n",
      "epoch: 29/1000, train loss: 16.0837, val loss: 16.1098\n",
      "epoch: 30/1000, train loss: 15.8624, val loss: 15.8938\n",
      "epoch: 31/1000, train loss: 15.6732, val loss: 15.7067\n",
      "epoch: 32/1000, train loss: 15.4941, val loss: 15.5318\n",
      "epoch: 33/1000, train loss: 15.3301, val loss: 15.3655\n",
      "epoch: 34/1000, train loss: 15.1709, val loss: 15.1980\n",
      "epoch: 35/1000, train loss: 15.0220, val loss: 15.0395\n",
      "epoch: 36/1000, train loss: 14.8792, val loss: 14.8727\n",
      "epoch: 37/1000, train loss: 14.7398, val loss: 14.7248\n",
      "epoch: 38/1000, train loss: 14.6065, val loss: 14.5886\n",
      "epoch: 39/1000, train loss: 14.4750, val loss: 14.4616\n",
      "epoch: 40/1000, train loss: 14.3601, val loss: 14.3449\n",
      "epoch: 41/1000, train loss: 14.2375, val loss: 14.2381\n",
      "epoch: 42/1000, train loss: 14.1232, val loss: 14.1393\n",
      "epoch: 43/1000, train loss: 14.0075, val loss: 14.0492\n",
      "epoch: 44/1000, train loss: 13.9046, val loss: 13.9579\n",
      "epoch: 45/1000, train loss: 13.8056, val loss: 13.8493\n",
      "epoch: 46/1000, train loss: 13.7107, val loss: 13.7473\n",
      "epoch: 47/1000, train loss: 13.6153, val loss: 13.6421\n",
      "epoch: 48/1000, train loss: 13.5276, val loss: 13.5497\n",
      "epoch: 49/1000, train loss: 13.4392, val loss: 13.4486\n",
      "epoch: 50/1000, train loss: 13.3542, val loss: 13.3548\n",
      "epoch: 51/1000, train loss: 13.2748, val loss: 13.2718\n",
      "epoch: 52/1000, train loss: 13.1972, val loss: 13.1849\n",
      "epoch: 53/1000, train loss: 13.1223, val loss: 13.1019\n",
      "epoch: 54/1000, train loss: 13.0428, val loss: 13.0153\n",
      "epoch: 55/1000, train loss: 12.9721, val loss: 12.9379\n",
      "epoch: 56/1000, train loss: 12.9036, val loss: 12.8523\n",
      "epoch: 57/1000, train loss: 12.8367, val loss: 12.7815\n",
      "epoch: 58/1000, train loss: 12.7698, val loss: 12.7100\n",
      "epoch: 59/1000, train loss: 12.7020, val loss: 12.6405\n",
      "epoch: 60/1000, train loss: 12.6362, val loss: 12.5776\n",
      "epoch: 61/1000, train loss: 12.5612, val loss: 12.5171\n",
      "epoch: 62/1000, train loss: 12.4187, val loss: 12.4819\n",
      "epoch: 63/1000, train loss: 12.2961, val loss: 12.4200\n",
      "epoch: 64/1000, train loss: 12.2437, val loss: 12.3421\n",
      "epoch: 65/1000, train loss: 12.2144, val loss: 12.2963\n",
      "epoch: 66/1000, train loss: 12.1875, val loss: 12.2550\n",
      "epoch: 67/1000, train loss: 12.1550, val loss: 12.2004\n",
      "epoch: 68/1000, train loss: 12.1131, val loss: 12.1459\n",
      "epoch: 69/1000, train loss: 12.0624, val loss: 12.1047\n",
      "epoch: 70/1000, train loss: 12.0159, val loss: 12.0471\n",
      "epoch: 71/1000, train loss: 11.9631, val loss: 12.0241\n",
      "epoch: 72/1000, train loss: 11.9195, val loss: 11.9873\n",
      "epoch: 73/1000, train loss: 11.8738, val loss: 11.9534\n",
      "epoch: 74/1000, train loss: 11.8323, val loss: 11.9121\n",
      "epoch: 75/1000, train loss: 11.7887, val loss: 11.8801\n",
      "epoch: 76/1000, train loss: 11.7455, val loss: 11.8286\n",
      "epoch: 77/1000, train loss: 11.7052, val loss: 11.7942\n",
      "epoch: 78/1000, train loss: 11.6615, val loss: 11.7631\n",
      "epoch: 79/1000, train loss: 11.6276, val loss: 11.7272\n",
      "epoch: 80/1000, train loss: 11.5889, val loss: 11.7226\n",
      "epoch: 81/1000, train loss: 11.5445, val loss: 11.6823\n",
      "epoch: 82/1000, train loss: 11.5106, val loss: 11.6395\n",
      "epoch: 83/1000, train loss: 11.4649, val loss: 11.5856\n",
      "epoch: 84/1000, train loss: 11.4256, val loss: 11.5727\n",
      "epoch: 85/1000, train loss: 11.3972, val loss: 11.5626\n",
      "epoch: 86/1000, train loss: 11.3691, val loss: 11.5308\n",
      "epoch: 87/1000, train loss: 11.3352, val loss: 11.5227\n",
      "epoch: 88/1000, train loss: 11.3040, val loss: 11.5067\n",
      "epoch: 89/1000, train loss: 11.2707, val loss: 11.4824\n",
      "epoch: 90/1000, train loss: 11.2375, val loss: 11.4619\n",
      "epoch: 91/1000, train loss: 11.2083, val loss: 11.4256\n",
      "epoch: 92/1000, train loss: 11.1779, val loss: 11.4455\n",
      "epoch: 93/1000, train loss: 11.1550, val loss: 11.4158\n",
      "epoch: 94/1000, train loss: 11.1279, val loss: 11.4376\n",
      "epoch: 95/1000, train loss: 11.1102, val loss: 11.4088\n",
      "epoch: 96/1000, train loss: 11.0896, val loss: 11.3473\n",
      "epoch: 97/1000, train loss: 11.0478, val loss: 11.3762\n",
      "epoch: 98/1000, train loss: 11.0307, val loss: 11.3788\n",
      "epoch: 99/1000, train loss: 11.0128, val loss: 11.3394\n",
      "epoch: 100/1000, train loss: 10.9812, val loss: 11.3642\n",
      "epoch: 101/1000, train loss: 10.9695, val loss: 11.3195\n",
      "epoch: 102/1000, train loss: 10.9378, val loss: 11.3508\n",
      "epoch: 103/1000, train loss: 10.9197, val loss: 11.3520\n",
      "epoch: 104/1000, train loss: 10.9051, val loss: 11.3410\n",
      "epoch: 105/1000, train loss: 10.8812, val loss: 11.3471\n",
      "epoch: 106/1000, train loss: 10.8561, val loss: 11.3335\n",
      "epoch: 107/1000, train loss: 10.8352, val loss: 11.3584\n",
      "epoch: 108/1000, train loss: 10.8155, val loss: 11.3602\n",
      "epoch: 109/1000, train loss: 10.7892, val loss: 11.3567\n",
      "epoch: 110/1000, train loss: 10.7707, val loss: 11.3465\n",
      "epoch: 111/1000, train loss: 10.7510, val loss: 11.3600\n",
      "epoch: 112/1000, train loss: 10.7341, val loss: 11.3392\n",
      "epoch: 113/1000, train loss: 10.7149, val loss: 11.3275\n",
      "epoch: 114/1000, train loss: 10.6923, val loss: 11.3303\n",
      "epoch: 115/1000, train loss: 10.6790, val loss: 11.3404\n",
      "epoch: 116/1000, train loss: 10.6589, val loss: 11.2942\n",
      "epoch: 117/1000, train loss: 10.6365, val loss: 11.2957\n",
      "epoch: 118/1000, train loss: 10.6194, val loss: 11.2770\n",
      "epoch: 119/1000, train loss: 10.6039, val loss: 11.2889\n",
      "epoch: 120/1000, train loss: 10.5907, val loss: 11.2975\n",
      "epoch: 121/1000, train loss: 10.5751, val loss: 11.2781\n",
      "epoch: 122/1000, train loss: 10.5517, val loss: 11.2812\n",
      "epoch: 123/1000, train loss: 10.5373, val loss: 11.2847\n",
      "epoch: 124/1000, train loss: 10.5173, val loss: 11.2617\n",
      "epoch: 125/1000, train loss: 10.5021, val loss: 11.2717\n",
      "epoch: 126/1000, train loss: 10.4910, val loss: 11.2433\n",
      "epoch: 127/1000, train loss: 10.4755, val loss: 11.1896\n",
      "epoch: 128/1000, train loss: 10.4596, val loss: 11.1678\n",
      "epoch: 129/1000, train loss: 10.4459, val loss: 11.1736\n",
      "epoch: 130/1000, train loss: 10.4301, val loss: 11.1406\n",
      "epoch: 131/1000, train loss: 10.4142, val loss: 11.1674\n",
      "epoch: 132/1000, train loss: 10.3979, val loss: 11.1518\n",
      "epoch: 133/1000, train loss: 10.3806, val loss: 11.1372\n",
      "epoch: 134/1000, train loss: 10.3722, val loss: 11.0671\n",
      "epoch: 135/1000, train loss: 10.3554, val loss: 11.1092\n",
      "epoch: 136/1000, train loss: 10.3375, val loss: 11.0197\n",
      "epoch: 137/1000, train loss: 10.3290, val loss: 11.0534\n",
      "epoch: 138/1000, train loss: 10.3134, val loss: 11.0233\n",
      "epoch: 139/1000, train loss: 10.2990, val loss: 11.0128\n",
      "epoch: 140/1000, train loss: 10.2879, val loss: 10.9899\n",
      "epoch: 141/1000, train loss: 10.2766, val loss: 11.0134\n",
      "epoch: 142/1000, train loss: 10.2605, val loss: 11.0011\n",
      "epoch: 143/1000, train loss: 10.2492, val loss: 10.9835\n",
      "epoch: 144/1000, train loss: 10.2345, val loss: 10.9856\n",
      "epoch: 145/1000, train loss: 10.2201, val loss: 10.9227\n",
      "epoch: 146/1000, train loss: 10.2128, val loss: 10.9549\n",
      "epoch: 147/1000, train loss: 10.1986, val loss: 10.9082\n",
      "epoch: 148/1000, train loss: 10.1843, val loss: 10.9098\n",
      "epoch: 149/1000, train loss: 10.1739, val loss: 10.8601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150/1000, train loss: 10.1605, val loss: 10.8520\n",
      "epoch: 151/1000, train loss: 10.1504, val loss: 10.8420\n",
      "epoch: 152/1000, train loss: 10.1367, val loss: 10.8303\n",
      "epoch: 153/1000, train loss: 10.1246, val loss: 10.8006\n",
      "epoch: 154/1000, train loss: 10.1139, val loss: 10.7852\n",
      "epoch: 155/1000, train loss: 10.0988, val loss: 10.7660\n",
      "epoch: 156/1000, train loss: 10.0875, val loss: 10.7744\n",
      "epoch: 157/1000, train loss: 10.0800, val loss: 10.7748\n",
      "epoch: 158/1000, train loss: 10.0638, val loss: 10.7554\n",
      "epoch: 159/1000, train loss: 10.0564, val loss: 10.7288\n",
      "epoch: 160/1000, train loss: 10.0424, val loss: 10.8019\n",
      "epoch: 161/1000, train loss: 10.0343, val loss: 10.7309\n",
      "epoch: 162/1000, train loss: 10.0196, val loss: 10.7326\n",
      "epoch: 163/1000, train loss: 10.0079, val loss: 10.6902\n",
      "epoch: 164/1000, train loss: 9.9948, val loss: 10.7110\n",
      "epoch: 165/1000, train loss: 9.9864, val loss: 10.6834\n",
      "epoch: 166/1000, train loss: 9.9694, val loss: 10.6327\n",
      "epoch: 167/1000, train loss: 9.9622, val loss: 10.6816\n",
      "epoch: 168/1000, train loss: 9.9480, val loss: 10.6360\n",
      "epoch: 169/1000, train loss: 9.9416, val loss: 10.6116\n",
      "epoch: 170/1000, train loss: 9.9259, val loss: 10.5464\n",
      "epoch: 171/1000, train loss: 9.9153, val loss: 10.5487\n",
      "epoch: 172/1000, train loss: 9.9033, val loss: 10.4976\n",
      "epoch: 173/1000, train loss: 9.8947, val loss: 10.4960\n",
      "epoch: 174/1000, train loss: 9.8877, val loss: 10.4633\n",
      "epoch: 175/1000, train loss: 9.8708, val loss: 10.4064\n",
      "epoch: 176/1000, train loss: 9.8587, val loss: 10.3524\n",
      "epoch: 177/1000, train loss: 9.8524, val loss: 10.3439\n",
      "epoch: 178/1000, train loss: 9.8384, val loss: 10.2982\n",
      "epoch: 179/1000, train loss: 9.8226, val loss: 10.2963\n",
      "epoch: 180/1000, train loss: 9.8137, val loss: 10.2846\n",
      "epoch: 181/1000, train loss: 9.8003, val loss: 10.2828\n",
      "epoch: 182/1000, train loss: 9.7697, val loss: 10.2446\n",
      "epoch: 183/1000, train loss: 9.7548, val loss: 10.1936\n",
      "epoch: 184/1000, train loss: 9.5875, val loss: 9.9130\n",
      "epoch: 185/1000, train loss: 9.5578, val loss: 9.7877\n",
      "epoch: 186/1000, train loss: 9.5098, val loss: 9.7880\n",
      "epoch: 187/1000, train loss: 9.4900, val loss: 9.7719\n",
      "epoch: 188/1000, train loss: 9.5048, val loss: 9.7766\n",
      "epoch: 189/1000, train loss: 9.5390, val loss: 9.7738\n",
      "epoch: 190/1000, train loss: 9.5471, val loss: 9.7714\n",
      "epoch: 191/1000, train loss: 9.5367, val loss: 9.7657\n",
      "epoch: 192/1000, train loss: 9.5249, val loss: 9.7590\n",
      "epoch: 193/1000, train loss: 9.5213, val loss: 9.7580\n",
      "epoch: 194/1000, train loss: 9.5043, val loss: 9.7559\n",
      "epoch: 195/1000, train loss: 9.5012, val loss: 9.7384\n",
      "epoch: 196/1000, train loss: 9.4931, val loss: 9.7381\n",
      "epoch: 197/1000, train loss: 9.4790, val loss: 9.7225\n",
      "epoch: 198/1000, train loss: 9.4724, val loss: 9.7216\n",
      "epoch: 199/1000, train loss: 9.4630, val loss: 9.7213\n",
      "epoch: 200/1000, train loss: 9.4564, val loss: 9.7079\n",
      "epoch: 201/1000, train loss: 9.4432, val loss: 9.7056\n",
      "epoch: 202/1000, train loss: 9.4338, val loss: 9.6878\n",
      "epoch: 203/1000, train loss: 9.4217, val loss: 9.6870\n",
      "epoch: 204/1000, train loss: 9.4153, val loss: 9.6866\n",
      "epoch: 205/1000, train loss: 9.4063, val loss: 9.6832\n",
      "epoch: 206/1000, train loss: 9.4001, val loss: 9.6705\n",
      "epoch: 207/1000, train loss: 9.3909, val loss: 9.6689\n",
      "epoch: 208/1000, train loss: 9.3779, val loss: 9.6566\n",
      "epoch: 209/1000, train loss: 9.3696, val loss: 9.6620\n",
      "epoch: 210/1000, train loss: 9.3677, val loss: 9.6529\n",
      "epoch: 211/1000, train loss: 9.3519, val loss: 9.6483\n",
      "epoch: 212/1000, train loss: 9.3409, val loss: 9.6404\n",
      "epoch: 213/1000, train loss: 9.3449, val loss: 9.6224\n",
      "epoch: 214/1000, train loss: 9.3343, val loss: 9.6210\n",
      "epoch: 215/1000, train loss: 9.3280, val loss: 9.6121\n",
      "epoch: 216/1000, train loss: 9.3235, val loss: 9.6076\n",
      "epoch: 217/1000, train loss: 9.3075, val loss: 9.6035\n",
      "epoch: 218/1000, train loss: 9.3042, val loss: 9.5993\n",
      "epoch: 219/1000, train loss: 9.3029, val loss: 9.6021\n",
      "epoch: 220/1000, train loss: 9.2889, val loss: 9.5937\n",
      "epoch: 221/1000, train loss: 9.2864, val loss: 9.5924\n",
      "epoch: 222/1000, train loss: 9.2800, val loss: 9.5861\n",
      "epoch: 223/1000, train loss: 9.2707, val loss: 9.5749\n",
      "epoch: 224/1000, train loss: 9.2645, val loss: 9.5780\n",
      "epoch: 225/1000, train loss: 9.2600, val loss: 9.5680\n",
      "epoch: 226/1000, train loss: 9.2544, val loss: 9.5626\n",
      "epoch: 227/1000, train loss: 9.2468, val loss: 9.5617\n",
      "epoch: 228/1000, train loss: 9.2483, val loss: 9.5602\n",
      "epoch: 229/1000, train loss: 9.2307, val loss: 9.5372\n",
      "epoch: 230/1000, train loss: 9.2294, val loss: 9.5367\n",
      "epoch: 231/1000, train loss: 9.2306, val loss: 9.5429\n",
      "epoch: 232/1000, train loss: 9.2168, val loss: 9.5272\n",
      "epoch: 233/1000, train loss: 9.2144, val loss: 9.5279\n",
      "epoch: 234/1000, train loss: 9.2071, val loss: 9.5236\n",
      "epoch: 235/1000, train loss: 9.2072, val loss: 9.5166\n",
      "epoch: 236/1000, train loss: 9.1972, val loss: 9.5078\n",
      "epoch: 237/1000, train loss: 9.1937, val loss: 9.5129\n",
      "epoch: 238/1000, train loss: 9.1783, val loss: 9.4992\n",
      "epoch: 239/1000, train loss: 9.1838, val loss: 9.4875\n",
      "epoch: 240/1000, train loss: 9.1734, val loss: 9.4960\n",
      "epoch: 241/1000, train loss: 9.1718, val loss: 9.4842\n",
      "epoch: 242/1000, train loss: 9.1601, val loss: 9.4771\n",
      "epoch: 243/1000, train loss: 9.1604, val loss: 9.4774\n",
      "epoch: 244/1000, train loss: 9.1565, val loss: 9.4767\n",
      "epoch: 245/1000, train loss: 9.1505, val loss: 9.4636\n",
      "epoch: 246/1000, train loss: 9.1392, val loss: 9.4635\n",
      "epoch: 247/1000, train loss: 9.1384, val loss: 9.4490\n",
      "epoch: 248/1000, train loss: 9.1331, val loss: 9.4496\n",
      "epoch: 249/1000, train loss: 9.1359, val loss: 9.4356\n",
      "epoch: 250/1000, train loss: 9.1153, val loss: 9.4318\n",
      "epoch: 251/1000, train loss: 9.1243, val loss: 9.4254\n",
      "epoch: 252/1000, train loss: 9.1206, val loss: 9.4182\n",
      "epoch: 253/1000, train loss: 9.1176, val loss: 9.4104\n",
      "epoch: 254/1000, train loss: 9.1087, val loss: 9.4047\n",
      "epoch: 255/1000, train loss: 9.1037, val loss: 9.3993\n",
      "epoch: 256/1000, train loss: 9.1007, val loss: 9.3989\n",
      "epoch: 257/1000, train loss: 9.0952, val loss: 9.3936\n",
      "epoch: 258/1000, train loss: 9.0971, val loss: 9.3924\n",
      "epoch: 259/1000, train loss: 9.0945, val loss: 9.3926\n",
      "epoch: 260/1000, train loss: 9.0832, val loss: 9.3811\n",
      "epoch: 261/1000, train loss: 9.0844, val loss: 9.3829\n",
      "epoch: 262/1000, train loss: 9.0817, val loss: 9.3796\n",
      "epoch: 263/1000, train loss: 9.0662, val loss: 9.3647\n",
      "epoch: 264/1000, train loss: 9.0770, val loss: 9.3606\n",
      "epoch: 265/1000, train loss: 9.0679, val loss: 9.3591\n",
      "epoch: 266/1000, train loss: 9.0595, val loss: 9.3543\n",
      "epoch: 267/1000, train loss: 9.0566, val loss: 9.3505\n",
      "epoch: 268/1000, train loss: 9.0592, val loss: 9.3431\n",
      "epoch: 269/1000, train loss: 9.0511, val loss: 9.3353\n",
      "epoch: 270/1000, train loss: 9.0463, val loss: 9.3311\n",
      "epoch: 271/1000, train loss: 9.0472, val loss: 9.3270\n",
      "epoch: 272/1000, train loss: 9.0359, val loss: 9.3212\n",
      "epoch: 273/1000, train loss: 9.0304, val loss: 9.3230\n",
      "epoch: 274/1000, train loss: 9.0367, val loss: 9.3132\n",
      "epoch: 275/1000, train loss: 9.0314, val loss: 9.3023\n",
      "epoch: 276/1000, train loss: 9.0171, val loss: 9.3002\n",
      "epoch: 277/1000, train loss: 9.0087, val loss: 9.3119\n",
      "epoch: 278/1000, train loss: 9.0125, val loss: 9.2982\n",
      "epoch: 279/1000, train loss: 9.0010, val loss: 9.2963\n",
      "epoch: 280/1000, train loss: 9.0001, val loss: 9.2933\n",
      "epoch: 281/1000, train loss: 8.9961, val loss: 9.2986\n",
      "epoch: 282/1000, train loss: 8.9919, val loss: 9.2929\n",
      "epoch: 283/1000, train loss: 8.9927, val loss: 9.2846\n",
      "epoch: 284/1000, train loss: 8.9828, val loss: 9.2890\n",
      "epoch: 285/1000, train loss: 8.9892, val loss: 9.2773\n",
      "epoch: 286/1000, train loss: 8.9809, val loss: 9.2688\n",
      "epoch: 287/1000, train loss: 8.9848, val loss: 9.2651\n",
      "epoch: 288/1000, train loss: 8.9671, val loss: 9.2607\n",
      "epoch: 289/1000, train loss: 8.9746, val loss: 9.2501\n",
      "epoch: 290/1000, train loss: 8.9688, val loss: 9.2521\n",
      "epoch: 291/1000, train loss: 8.9659, val loss: 9.2494\n",
      "epoch: 292/1000, train loss: 8.9490, val loss: 9.2445\n",
      "epoch: 293/1000, train loss: 8.9580, val loss: 9.2401\n",
      "epoch: 294/1000, train loss: 8.9541, val loss: 9.2363\n",
      "epoch: 295/1000, train loss: 8.9493, val loss: 9.2321\n",
      "epoch: 296/1000, train loss: 8.9512, val loss: 9.2236\n",
      "epoch: 297/1000, train loss: 8.9438, val loss: 9.2356\n",
      "epoch: 298/1000, train loss: 8.9416, val loss: 9.2196\n",
      "epoch: 299/1000, train loss: 8.9373, val loss: 9.2194\n",
      "epoch: 300/1000, train loss: 8.9349, val loss: 9.2123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 301/1000, train loss: 8.9249, val loss: 9.2119\n",
      "epoch: 302/1000, train loss: 8.9219, val loss: 9.2052\n",
      "epoch: 303/1000, train loss: 8.9231, val loss: 9.2076\n",
      "epoch: 304/1000, train loss: 8.9188, val loss: 9.1989\n",
      "epoch: 305/1000, train loss: 8.9213, val loss: 9.1893\n",
      "epoch: 306/1000, train loss: 8.9079, val loss: 9.1912\n",
      "epoch: 307/1000, train loss: 8.9070, val loss: 9.1937\n",
      "epoch: 308/1000, train loss: 8.9085, val loss: 9.1940\n",
      "epoch: 309/1000, train loss: 8.9024, val loss: 9.1857\n",
      "epoch: 310/1000, train loss: 8.9043, val loss: 9.1769\n",
      "epoch: 311/1000, train loss: 8.9012, val loss: 9.1695\n",
      "epoch: 312/1000, train loss: 8.8991, val loss: 9.1720\n",
      "epoch: 313/1000, train loss: 8.8872, val loss: 9.1637\n",
      "epoch: 314/1000, train loss: 8.8799, val loss: 9.1765\n",
      "epoch: 315/1000, train loss: 8.8778, val loss: 9.1654\n",
      "epoch: 316/1000, train loss: 8.8787, val loss: 9.1698\n",
      "epoch: 317/1000, train loss: 8.8757, val loss: 9.1593\n",
      "epoch: 318/1000, train loss: 8.8659, val loss: 9.1683\n",
      "epoch: 319/1000, train loss: 8.8754, val loss: 9.1599\n",
      "epoch: 320/1000, train loss: 8.8688, val loss: 9.1603\n",
      "epoch: 321/1000, train loss: 8.8623, val loss: 9.1587\n",
      "epoch: 322/1000, train loss: 8.8574, val loss: 9.1513\n",
      "epoch: 323/1000, train loss: 8.8574, val loss: 9.1574\n",
      "epoch: 324/1000, train loss: 8.8504, val loss: 9.1550\n",
      "epoch: 325/1000, train loss: 8.8447, val loss: 9.1524\n",
      "epoch: 326/1000, train loss: 8.8484, val loss: 9.1501\n",
      "epoch: 327/1000, train loss: 8.8496, val loss: 9.1411\n",
      "epoch: 328/1000, train loss: 8.8426, val loss: 9.1459\n",
      "epoch: 329/1000, train loss: 8.8428, val loss: 9.1398\n",
      "epoch: 330/1000, train loss: 8.8432, val loss: 9.1418\n",
      "epoch: 331/1000, train loss: 8.8359, val loss: 9.1353\n",
      "epoch: 332/1000, train loss: 8.8379, val loss: 9.1421\n",
      "epoch: 333/1000, train loss: 8.8344, val loss: 9.1371\n",
      "epoch: 334/1000, train loss: 8.8303, val loss: 9.1359\n",
      "epoch: 335/1000, train loss: 8.8208, val loss: 9.1367\n",
      "epoch: 336/1000, train loss: 8.8205, val loss: 9.1341\n",
      "epoch: 337/1000, train loss: 8.8224, val loss: 9.1346\n",
      "epoch: 338/1000, train loss: 8.8203, val loss: 9.1351\n",
      "epoch: 339/1000, train loss: 8.8082, val loss: 9.1301\n",
      "epoch: 340/1000, train loss: 8.8110, val loss: 9.1236\n",
      "epoch: 341/1000, train loss: 8.8037, val loss: 9.1258\n",
      "epoch: 342/1000, train loss: 8.8048, val loss: 9.1238\n",
      "epoch: 343/1000, train loss: 8.8040, val loss: 9.1308\n",
      "epoch: 344/1000, train loss: 8.8035, val loss: 9.1218\n",
      "epoch: 345/1000, train loss: 8.7980, val loss: 9.1284\n",
      "epoch: 346/1000, train loss: 8.8009, val loss: 9.1217\n",
      "epoch: 347/1000, train loss: 8.7925, val loss: 9.1200\n",
      "epoch: 348/1000, train loss: 8.7887, val loss: 9.1279\n",
      "epoch: 349/1000, train loss: 8.7880, val loss: 9.1212\n",
      "epoch: 350/1000, train loss: 8.7803, val loss: 9.1173\n",
      "epoch: 351/1000, train loss: 8.7887, val loss: 9.1094\n",
      "epoch: 352/1000, train loss: 8.7738, val loss: 9.1089\n",
      "epoch: 353/1000, train loss: 8.7866, val loss: 9.1187\n",
      "epoch: 354/1000, train loss: 8.7645, val loss: 9.1031\n",
      "epoch: 355/1000, train loss: 8.7757, val loss: 9.1122\n",
      "epoch: 356/1000, train loss: 8.7744, val loss: 9.0918\n",
      "epoch: 357/1000, train loss: 8.7705, val loss: 9.1003\n",
      "epoch: 358/1000, train loss: 8.7571, val loss: 9.0947\n",
      "epoch: 359/1000, train loss: 8.7626, val loss: 9.0865\n",
      "epoch: 360/1000, train loss: 8.7575, val loss: 9.0872\n",
      "epoch: 361/1000, train loss: 8.7547, val loss: 9.0856\n",
      "epoch: 362/1000, train loss: 8.7526, val loss: 9.0915\n",
      "epoch: 363/1000, train loss: 8.7531, val loss: 9.0790\n",
      "epoch: 364/1000, train loss: 8.7483, val loss: 9.0820\n",
      "epoch: 365/1000, train loss: 8.7469, val loss: 9.0843\n",
      "epoch: 366/1000, train loss: 8.7432, val loss: 9.0895\n",
      "epoch: 367/1000, train loss: 8.7502, val loss: 9.0873\n",
      "epoch: 368/1000, train loss: 8.7367, val loss: 9.0764\n",
      "epoch: 369/1000, train loss: 8.7415, val loss: 9.0881\n",
      "epoch: 370/1000, train loss: 8.7342, val loss: 9.0874\n",
      "epoch: 371/1000, train loss: 8.7302, val loss: 9.0864\n",
      "epoch: 372/1000, train loss: 8.7225, val loss: 9.0801\n",
      "epoch: 373/1000, train loss: 8.7383, val loss: 9.0800\n",
      "epoch: 374/1000, train loss: 8.7285, val loss: 9.0750\n",
      "epoch: 375/1000, train loss: 8.7132, val loss: 9.0631\n",
      "epoch: 376/1000, train loss: 8.7370, val loss: 9.0828\n",
      "epoch: 377/1000, train loss: 8.7136, val loss: 9.0776\n",
      "epoch: 378/1000, train loss: 8.7161, val loss: 9.0670\n",
      "epoch: 379/1000, train loss: 8.7065, val loss: 9.0699\n",
      "epoch: 380/1000, train loss: 8.7154, val loss: 9.0633\n",
      "epoch: 381/1000, train loss: 8.7052, val loss: 9.0644\n",
      "epoch: 382/1000, train loss: 8.7077, val loss: 9.0631\n",
      "epoch: 383/1000, train loss: 8.7201, val loss: 9.0684\n",
      "epoch: 384/1000, train loss: 8.6931, val loss: 9.0586\n",
      "epoch: 385/1000, train loss: 8.6894, val loss: 9.0565\n",
      "epoch: 386/1000, train loss: 8.6861, val loss: 9.0357\n",
      "epoch: 387/1000, train loss: 8.7088, val loss: 9.0341\n",
      "epoch: 388/1000, train loss: 8.6836, val loss: 9.0375\n",
      "epoch: 389/1000, train loss: 8.6882, val loss: 9.0318\n",
      "epoch: 390/1000, train loss: 8.7142, val loss: 9.0825\n",
      "epoch: 391/1000, train loss: 8.6787, val loss: 9.0515\n",
      "epoch: 392/1000, train loss: 8.6815, val loss: 9.0487\n",
      "epoch: 393/1000, train loss: 8.6777, val loss: 9.0398\n",
      "epoch: 394/1000, train loss: 8.6778, val loss: 9.0342\n",
      "epoch: 395/1000, train loss: 8.6837, val loss: 9.0292\n",
      "epoch: 396/1000, train loss: 8.6694, val loss: 9.0344\n",
      "epoch: 397/1000, train loss: 8.6642, val loss: 9.0146\n",
      "epoch: 398/1000, train loss: 8.6943, val loss: 9.0590\n",
      "epoch: 399/1000, train loss: 8.6684, val loss: 9.0177\n",
      "epoch: 400/1000, train loss: 8.6725, val loss: 9.0455\n",
      "epoch: 401/1000, train loss: 8.6576, val loss: 9.0183\n",
      "epoch: 402/1000, train loss: 8.6618, val loss: 9.0094\n",
      "epoch: 403/1000, train loss: 8.6606, val loss: 9.0292\n",
      "epoch: 404/1000, train loss: 8.6659, val loss: 9.0274\n",
      "epoch: 405/1000, train loss: 8.6486, val loss: 9.0224\n",
      "epoch: 406/1000, train loss: 8.6547, val loss: 9.0227\n",
      "epoch: 407/1000, train loss: 8.6503, val loss: 9.0194\n",
      "epoch: 408/1000, train loss: 8.6527, val loss: 9.0255\n",
      "epoch: 409/1000, train loss: 8.6514, val loss: 9.0234\n",
      "epoch: 410/1000, train loss: 8.6412, val loss: 9.0198\n",
      "epoch: 411/1000, train loss: 8.6425, val loss: 9.0085\n",
      "epoch: 412/1000, train loss: 8.6317, val loss: 9.0079\n",
      "epoch: 413/1000, train loss: 8.6413, val loss: 8.9970\n",
      "epoch: 414/1000, train loss: 8.6429, val loss: 9.0014\n",
      "epoch: 415/1000, train loss: 8.6260, val loss: 9.0066\n",
      "epoch: 416/1000, train loss: 8.6442, val loss: 8.9988\n",
      "epoch: 417/1000, train loss: 8.6188, val loss: 8.9976\n",
      "epoch: 418/1000, train loss: 8.6345, val loss: 8.9931\n",
      "epoch: 419/1000, train loss: 8.6454, val loss: 9.0386\n",
      "epoch: 420/1000, train loss: 8.6257, val loss: 8.9892\n",
      "epoch: 421/1000, train loss: 8.6246, val loss: 9.0037\n",
      "epoch: 422/1000, train loss: 8.6133, val loss: 8.9851\n",
      "epoch: 423/1000, train loss: 8.6209, val loss: 8.9776\n",
      "epoch: 424/1000, train loss: 8.6071, val loss: 8.9564\n",
      "epoch: 425/1000, train loss: 8.6454, val loss: 9.0152\n",
      "epoch: 426/1000, train loss: 8.6224, val loss: 9.0030\n",
      "epoch: 427/1000, train loss: 8.6160, val loss: 8.9743\n",
      "epoch: 428/1000, train loss: 8.6069, val loss: 8.9761\n",
      "epoch: 429/1000, train loss: 8.6079, val loss: 8.9942\n",
      "epoch: 430/1000, train loss: 8.6128, val loss: 8.9688\n",
      "epoch: 431/1000, train loss: 8.6093, val loss: 8.9815\n",
      "epoch: 432/1000, train loss: 8.6080, val loss: 8.9889\n",
      "epoch: 433/1000, train loss: 8.6076, val loss: 8.9575\n",
      "epoch: 434/1000, train loss: 8.6041, val loss: 8.9856\n",
      "epoch: 435/1000, train loss: 8.6057, val loss: 8.9802\n",
      "epoch: 436/1000, train loss: 8.5936, val loss: 8.9862\n",
      "epoch: 437/1000, train loss: 8.6023, val loss: 8.9862\n",
      "epoch: 438/1000, train loss: 8.5909, val loss: 8.9597\n",
      "epoch: 439/1000, train loss: 8.5958, val loss: 8.9812\n",
      "epoch: 440/1000, train loss: 8.5949, val loss: 8.9886\n",
      "epoch: 441/1000, train loss: 8.5913, val loss: 8.9775\n",
      "epoch: 442/1000, train loss: 8.5843, val loss: 8.9684\n",
      "epoch: 443/1000, train loss: 8.5851, val loss: 8.9794\n",
      "epoch: 444/1000, train loss: 8.5913, val loss: 8.9844\n",
      "epoch: 445/1000, train loss: 8.5893, val loss: 8.9896\n",
      "epoch: 446/1000, train loss: 8.5915, val loss: 9.0027\n",
      "epoch: 447/1000, train loss: 8.5893, val loss: 9.0109\n",
      "epoch: 448/1000, train loss: 8.5789, val loss: 8.9976\n",
      "epoch: 449/1000, train loss: 8.5761, val loss: 9.0024\n",
      "epoch: 450/1000, train loss: 8.5704, val loss: 8.9829\n",
      "epoch: 451/1000, train loss: 8.5642, val loss: 8.9939\n",
      "epoch: 452/1000, train loss: 8.5755, val loss: 8.9902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 453/1000, train loss: 8.5681, val loss: 8.9717\n",
      "epoch: 454/1000, train loss: 8.5636, val loss: 8.9917\n",
      "epoch: 455/1000, train loss: 8.5661, val loss: 8.9788\n",
      "epoch: 456/1000, train loss: 8.5606, val loss: 8.9829\n",
      "epoch: 457/1000, train loss: 8.5569, val loss: 8.9659\n",
      "epoch: 458/1000, train loss: 8.5576, val loss: 8.9951\n",
      "epoch: 459/1000, train loss: 8.5637, val loss: 8.9735\n",
      "epoch: 460/1000, train loss: 8.5557, val loss: 8.9694\n",
      "epoch: 461/1000, train loss: 8.5526, val loss: 8.9518\n",
      "epoch: 462/1000, train loss: 8.5503, val loss: 8.9718\n",
      "epoch: 463/1000, train loss: 8.5456, val loss: 8.9604\n",
      "epoch: 464/1000, train loss: 8.5352, val loss: 8.9293\n",
      "epoch: 465/1000, train loss: 8.5348, val loss: 8.9388\n",
      "epoch: 466/1000, train loss: 8.5424, val loss: 8.9514\n",
      "epoch: 467/1000, train loss: 8.5403, val loss: 8.9367\n",
      "epoch: 468/1000, train loss: 8.5412, val loss: 8.9583\n",
      "epoch: 469/1000, train loss: 8.5360, val loss: 8.9355\n",
      "epoch: 470/1000, train loss: 8.5376, val loss: 8.9453\n",
      "epoch: 471/1000, train loss: 8.5227, val loss: 8.9509\n",
      "epoch: 472/1000, train loss: 8.5313, val loss: 8.9406\n",
      "epoch: 473/1000, train loss: 8.5278, val loss: 8.9336\n",
      "epoch: 474/1000, train loss: 8.5290, val loss: 8.9232\n",
      "epoch: 475/1000, train loss: 8.5235, val loss: 8.9226\n",
      "epoch: 476/1000, train loss: 8.5230, val loss: 8.9271\n",
      "epoch: 477/1000, train loss: 8.5092, val loss: 8.9127\n",
      "epoch: 478/1000, train loss: 8.5221, val loss: 8.9334\n",
      "epoch: 479/1000, train loss: 8.5130, val loss: 8.9222\n",
      "epoch: 480/1000, train loss: 8.5169, val loss: 8.9464\n",
      "epoch: 481/1000, train loss: 8.5088, val loss: 8.9053\n",
      "epoch: 482/1000, train loss: 8.5077, val loss: 8.9117\n",
      "epoch: 483/1000, train loss: 8.5174, val loss: 8.9158\n",
      "epoch: 484/1000, train loss: 8.5079, val loss: 8.9234\n",
      "epoch: 485/1000, train loss: 8.4962, val loss: 8.9011\n",
      "epoch: 486/1000, train loss: 8.4996, val loss: 8.8816\n",
      "epoch: 487/1000, train loss: 8.5030, val loss: 8.9154\n",
      "epoch: 488/1000, train loss: 8.5004, val loss: 8.9120\n",
      "epoch: 489/1000, train loss: 8.4888, val loss: 8.8949\n",
      "epoch: 490/1000, train loss: 8.5041, val loss: 8.8854\n",
      "epoch: 491/1000, train loss: 8.4896, val loss: 8.8870\n",
      "epoch: 492/1000, train loss: 8.4861, val loss: 8.8669\n",
      "epoch: 493/1000, train loss: 8.4918, val loss: 8.8923\n",
      "epoch: 494/1000, train loss: 8.4862, val loss: 8.8726\n",
      "epoch: 495/1000, train loss: 8.4840, val loss: 8.8793\n",
      "epoch: 496/1000, train loss: 8.4792, val loss: 8.8710\n",
      "epoch: 497/1000, train loss: 8.4788, val loss: 8.8729\n",
      "epoch: 498/1000, train loss: 8.4843, val loss: 8.8643\n",
      "epoch: 499/1000, train loss: 8.4719, val loss: 8.8634\n",
      "epoch: 500/1000, train loss: 8.4781, val loss: 8.8787\n",
      "epoch: 501/1000, train loss: 8.4700, val loss: 8.8650\n",
      "epoch: 502/1000, train loss: 8.4800, val loss: 8.8766\n",
      "epoch: 503/1000, train loss: 8.4766, val loss: 8.8603\n",
      "epoch: 504/1000, train loss: 8.4538, val loss: 8.8589\n",
      "epoch: 505/1000, train loss: 8.4710, val loss: 8.8785\n",
      "epoch: 506/1000, train loss: 8.4729, val loss: 8.8582\n",
      "epoch: 507/1000, train loss: 8.4531, val loss: 8.8523\n",
      "epoch: 508/1000, train loss: 8.4572, val loss: 8.8506\n",
      "epoch: 509/1000, train loss: 8.4579, val loss: 8.8602\n",
      "epoch: 510/1000, train loss: 8.4544, val loss: 8.8466\n",
      "epoch: 511/1000, train loss: 8.4494, val loss: 8.8377\n",
      "epoch: 512/1000, train loss: 8.4574, val loss: 8.8636\n",
      "epoch: 513/1000, train loss: 8.4516, val loss: 8.8428\n",
      "epoch: 514/1000, train loss: 8.4510, val loss: 8.8414\n",
      "epoch: 515/1000, train loss: 8.4725, val loss: 8.8340\n",
      "epoch: 516/1000, train loss: 8.4470, val loss: 8.8237\n",
      "epoch: 517/1000, train loss: 8.4485, val loss: 8.8319\n",
      "epoch: 518/1000, train loss: 8.4320, val loss: 8.8261\n",
      "epoch: 519/1000, train loss: 8.4498, val loss: 8.8289\n",
      "epoch: 520/1000, train loss: 8.4562, val loss: 8.8350\n",
      "epoch: 521/1000, train loss: 8.4315, val loss: 8.8204\n",
      "epoch: 522/1000, train loss: 8.4366, val loss: 8.8317\n",
      "epoch: 523/1000, train loss: 8.4375, val loss: 8.8085\n",
      "epoch: 524/1000, train loss: 8.4384, val loss: 8.8174\n",
      "epoch: 525/1000, train loss: 8.4311, val loss: 8.8315\n",
      "epoch: 526/1000, train loss: 8.4325, val loss: 8.8178\n",
      "epoch: 527/1000, train loss: 8.4333, val loss: 8.8173\n",
      "epoch: 528/1000, train loss: 8.4372, val loss: 8.8078\n",
      "epoch: 529/1000, train loss: 8.4184, val loss: 8.8149\n",
      "epoch: 530/1000, train loss: 8.4345, val loss: 8.8080\n",
      "epoch: 531/1000, train loss: 8.4308, val loss: 8.7984\n",
      "epoch: 532/1000, train loss: 8.4192, val loss: 8.8137\n",
      "epoch: 533/1000, train loss: 8.4339, val loss: 8.8095\n",
      "epoch: 534/1000, train loss: 8.4236, val loss: 8.8042\n",
      "epoch: 535/1000, train loss: 8.4261, val loss: 8.8052\n",
      "epoch: 536/1000, train loss: 8.4196, val loss: 8.8015\n",
      "epoch: 537/1000, train loss: 8.4121, val loss: 8.7914\n",
      "epoch: 538/1000, train loss: 8.4210, val loss: 8.7893\n",
      "epoch: 539/1000, train loss: 8.4157, val loss: 8.7871\n",
      "epoch: 540/1000, train loss: 8.4181, val loss: 8.7972\n",
      "epoch: 541/1000, train loss: 8.4228, val loss: 8.7905\n",
      "epoch: 542/1000, train loss: 8.4384, val loss: 8.8346\n",
      "epoch: 543/1000, train loss: 8.4534, val loss: 8.8781\n",
      "epoch: 544/1000, train loss: 8.4291, val loss: 8.8605\n",
      "epoch: 545/1000, train loss: 8.4150, val loss: 8.8290\n",
      "epoch: 546/1000, train loss: 8.4164, val loss: 8.8448\n",
      "epoch: 547/1000, train loss: 8.3965, val loss: 8.8184\n",
      "epoch: 548/1000, train loss: 8.4036, val loss: 8.8050\n",
      "epoch: 549/1000, train loss: 8.3999, val loss: 8.7976\n",
      "epoch: 550/1000, train loss: 8.3892, val loss: 8.7733\n",
      "epoch: 551/1000, train loss: 8.3888, val loss: 8.7757\n",
      "epoch: 552/1000, train loss: 8.3843, val loss: 8.7624\n",
      "epoch: 553/1000, train loss: 8.3946, val loss: 8.7734\n",
      "epoch: 554/1000, train loss: 8.4019, val loss: 8.7772\n",
      "epoch: 555/1000, train loss: 8.3969, val loss: 8.7739\n",
      "epoch: 556/1000, train loss: 8.3959, val loss: 8.7780\n",
      "epoch: 557/1000, train loss: 8.3911, val loss: 8.7764\n",
      "epoch: 558/1000, train loss: 8.3838, val loss: 8.7646\n",
      "epoch: 559/1000, train loss: 8.3918, val loss: 8.7651\n",
      "epoch: 560/1000, train loss: 8.3845, val loss: 8.7624\n",
      "epoch: 561/1000, train loss: 8.3775, val loss: 8.7678\n",
      "epoch: 562/1000, train loss: 8.3906, val loss: 8.7663\n",
      "epoch: 563/1000, train loss: 8.3771, val loss: 8.7602\n",
      "epoch: 564/1000, train loss: 8.3814, val loss: 8.7704\n",
      "epoch: 565/1000, train loss: 8.3913, val loss: 8.7783\n",
      "epoch: 566/1000, train loss: 8.3883, val loss: 8.7719\n",
      "epoch: 567/1000, train loss: 8.3786, val loss: 8.7754\n",
      "epoch: 568/1000, train loss: 8.3782, val loss: 8.7710\n",
      "epoch: 569/1000, train loss: 8.3654, val loss: 8.7553\n",
      "epoch: 570/1000, train loss: 8.3741, val loss: 8.7514\n",
      "epoch: 571/1000, train loss: 8.3704, val loss: 8.7552\n",
      "epoch: 572/1000, train loss: 8.3795, val loss: 8.7679\n",
      "epoch: 573/1000, train loss: 8.3763, val loss: 8.7569\n",
      "epoch: 574/1000, train loss: 8.3634, val loss: 8.7518\n",
      "epoch: 575/1000, train loss: 8.3676, val loss: 8.7443\n",
      "epoch: 576/1000, train loss: 8.3637, val loss: 8.7412\n",
      "epoch: 577/1000, train loss: 8.3679, val loss: 8.7649\n",
      "epoch: 578/1000, train loss: 8.3653, val loss: 8.7538\n",
      "epoch: 579/1000, train loss: 8.3533, val loss: 8.7398\n",
      "epoch: 580/1000, train loss: 8.3624, val loss: 8.7542\n",
      "epoch: 581/1000, train loss: 8.3568, val loss: 8.7368\n",
      "epoch: 582/1000, train loss: 8.3539, val loss: 8.7366\n",
      "epoch: 583/1000, train loss: 8.3490, val loss: 8.7371\n",
      "epoch: 584/1000, train loss: 8.3504, val loss: 8.7332\n",
      "epoch: 585/1000, train loss: 8.3457, val loss: 8.7365\n",
      "epoch: 586/1000, train loss: 8.3441, val loss: 8.7215\n",
      "epoch: 587/1000, train loss: 8.3549, val loss: 8.7241\n",
      "epoch: 588/1000, train loss: 8.3613, val loss: 8.7435\n",
      "epoch: 589/1000, train loss: 8.3574, val loss: 8.7449\n",
      "epoch: 590/1000, train loss: 8.3555, val loss: 8.7478\n",
      "epoch: 591/1000, train loss: 8.3395, val loss: 8.7449\n",
      "epoch: 592/1000, train loss: 8.3564, val loss: 8.7515\n",
      "epoch: 593/1000, train loss: 8.3586, val loss: 8.7602\n",
      "epoch: 594/1000, train loss: 8.3536, val loss: 8.7658\n",
      "epoch: 595/1000, train loss: 8.3475, val loss: 8.7468\n",
      "epoch: 596/1000, train loss: 8.3326, val loss: 8.7508\n",
      "epoch: 597/1000, train loss: 8.3378, val loss: 8.7374\n",
      "epoch: 598/1000, train loss: 8.3233, val loss: 8.7192\n",
      "epoch: 599/1000, train loss: 8.3277, val loss: 8.7257\n",
      "epoch: 600/1000, train loss: 8.3278, val loss: 8.7162\n",
      "epoch: 601/1000, train loss: 8.3419, val loss: 8.7262\n",
      "epoch: 602/1000, train loss: 8.3350, val loss: 8.7284\n",
      "epoch: 603/1000, train loss: 8.3430, val loss: 8.7402\n",
      "epoch: 604/1000, train loss: 8.3370, val loss: 8.7471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 605/1000, train loss: 8.3262, val loss: 8.7423\n",
      "epoch: 606/1000, train loss: 8.3326, val loss: 8.7338\n",
      "epoch: 607/1000, train loss: 8.3305, val loss: 8.7377\n",
      "epoch: 608/1000, train loss: 8.3197, val loss: 8.7260\n",
      "epoch: 609/1000, train loss: 8.3260, val loss: 8.7305\n",
      "epoch: 610/1000, train loss: 8.3236, val loss: 8.7102\n",
      "epoch: 611/1000, train loss: 8.3113, val loss: 8.7128\n",
      "epoch: 612/1000, train loss: 8.3225, val loss: 8.7143\n",
      "epoch: 613/1000, train loss: 8.3158, val loss: 8.7189\n",
      "epoch: 614/1000, train loss: 8.3177, val loss: 8.7320\n",
      "epoch: 615/1000, train loss: 8.3197, val loss: 8.7206\n",
      "epoch: 616/1000, train loss: 8.3089, val loss: 8.7243\n",
      "epoch: 617/1000, train loss: 8.3107, val loss: 8.7226\n",
      "epoch: 618/1000, train loss: 8.3162, val loss: 8.7472\n",
      "epoch: 619/1000, train loss: 8.3128, val loss: 8.7260\n",
      "epoch: 620/1000, train loss: 8.3044, val loss: 8.7260\n",
      "epoch: 621/1000, train loss: 8.3108, val loss: 8.7244\n",
      "epoch: 622/1000, train loss: 8.3096, val loss: 8.7243\n",
      "epoch: 623/1000, train loss: 8.2969, val loss: 8.7296\n",
      "epoch: 624/1000, train loss: 8.3033, val loss: 8.7098\n",
      "epoch: 625/1000, train loss: 8.2912, val loss: 8.7071\n",
      "epoch: 626/1000, train loss: 8.2948, val loss: 8.7053\n",
      "epoch: 627/1000, train loss: 8.2928, val loss: 8.7007\n",
      "epoch: 628/1000, train loss: 8.2987, val loss: 8.7094\n",
      "epoch: 629/1000, train loss: 8.3004, val loss: 8.7370\n",
      "epoch: 630/1000, train loss: 8.2976, val loss: 8.7017\n",
      "epoch: 631/1000, train loss: 8.2924, val loss: 8.7049\n",
      "epoch: 632/1000, train loss: 8.2879, val loss: 8.7218\n",
      "epoch: 633/1000, train loss: 8.2935, val loss: 8.7037\n",
      "epoch: 634/1000, train loss: 8.2810, val loss: 8.6985\n",
      "epoch: 635/1000, train loss: 8.2850, val loss: 8.6903\n",
      "epoch: 636/1000, train loss: 8.2916, val loss: 8.6993\n",
      "epoch: 637/1000, train loss: 8.2948, val loss: 8.7099\n",
      "epoch: 638/1000, train loss: 8.2829, val loss: 8.6910\n",
      "epoch: 639/1000, train loss: 8.2801, val loss: 8.7099\n",
      "epoch: 640/1000, train loss: 8.2806, val loss: 8.7019\n",
      "epoch: 641/1000, train loss: 8.2810, val loss: 8.6953\n",
      "epoch: 642/1000, train loss: 8.2783, val loss: 8.6971\n",
      "epoch: 643/1000, train loss: 8.2772, val loss: 8.7043\n",
      "epoch: 644/1000, train loss: 8.2745, val loss: 8.7022\n",
      "epoch: 645/1000, train loss: 8.2740, val loss: 8.7014\n",
      "epoch: 646/1000, train loss: 8.2771, val loss: 8.7121\n",
      "epoch: 647/1000, train loss: 8.2847, val loss: 8.7210\n",
      "epoch: 648/1000, train loss: 8.2676, val loss: 8.7050\n",
      "epoch: 649/1000, train loss: 8.2666, val loss: 8.6936\n",
      "epoch: 650/1000, train loss: 8.2628, val loss: 8.6770\n",
      "epoch: 651/1000, train loss: 8.2647, val loss: 8.6736\n",
      "epoch: 652/1000, train loss: 8.2686, val loss: 8.6697\n",
      "epoch: 653/1000, train loss: 8.2680, val loss: 8.6805\n",
      "epoch: 654/1000, train loss: 8.2702, val loss: 8.6712\n",
      "epoch: 655/1000, train loss: 8.2590, val loss: 8.6667\n",
      "epoch: 656/1000, train loss: 8.2623, val loss: 8.6630\n",
      "epoch: 657/1000, train loss: 8.2667, val loss: 8.6722\n",
      "epoch: 658/1000, train loss: 8.2677, val loss: 8.6556\n",
      "epoch: 659/1000, train loss: 8.2524, val loss: 8.6688\n",
      "epoch: 660/1000, train loss: 8.2615, val loss: 8.6819\n",
      "epoch: 661/1000, train loss: 8.2564, val loss: 8.6633\n",
      "epoch: 662/1000, train loss: 8.2466, val loss: 8.6579\n",
      "epoch: 663/1000, train loss: 8.2577, val loss: 8.6536\n",
      "epoch: 664/1000, train loss: 8.2502, val loss: 8.6518\n",
      "epoch: 665/1000, train loss: 8.2560, val loss: 8.6519\n",
      "epoch: 666/1000, train loss: 8.2546, val loss: 8.6405\n",
      "epoch: 667/1000, train loss: 8.2531, val loss: 8.6423\n",
      "epoch: 668/1000, train loss: 8.2498, val loss: 8.6633\n",
      "epoch: 669/1000, train loss: 8.2488, val loss: 8.6429\n",
      "epoch: 670/1000, train loss: 8.2504, val loss: 8.6541\n",
      "epoch: 671/1000, train loss: 8.2588, val loss: 8.6616\n",
      "epoch: 672/1000, train loss: 8.2473, val loss: 8.6520\n",
      "epoch: 673/1000, train loss: 8.2389, val loss: 8.6515\n",
      "epoch: 674/1000, train loss: 8.2470, val loss: 8.6550\n",
      "epoch: 675/1000, train loss: 8.2373, val loss: 8.6332\n",
      "epoch: 676/1000, train loss: 8.2418, val loss: 8.6301\n",
      "epoch: 677/1000, train loss: 8.2340, val loss: 8.6248\n",
      "epoch: 678/1000, train loss: 8.2391, val loss: 8.6336\n",
      "epoch: 679/1000, train loss: 8.2428, val loss: 8.6442\n",
      "epoch: 680/1000, train loss: 8.2338, val loss: 8.6276\n",
      "epoch: 681/1000, train loss: 8.2369, val loss: 8.6489\n",
      "epoch: 682/1000, train loss: 8.2349, val loss: 8.6271\n",
      "epoch: 683/1000, train loss: 8.2308, val loss: 8.6423\n",
      "epoch: 684/1000, train loss: 8.2332, val loss: 8.6424\n",
      "epoch: 685/1000, train loss: 8.2323, val loss: 8.6320\n",
      "epoch: 686/1000, train loss: 8.2298, val loss: 8.6316\n",
      "epoch: 687/1000, train loss: 8.2307, val loss: 8.6281\n",
      "epoch: 688/1000, train loss: 8.2207, val loss: 8.6398\n",
      "epoch: 689/1000, train loss: 8.2322, val loss: 8.6405\n",
      "epoch: 690/1000, train loss: 8.2248, val loss: 8.6278\n",
      "epoch: 691/1000, train loss: 8.2243, val loss: 8.6429\n",
      "epoch: 692/1000, train loss: 8.2245, val loss: 8.6354\n",
      "epoch: 693/1000, train loss: 8.2184, val loss: 8.6185\n",
      "epoch: 694/1000, train loss: 8.2227, val loss: 8.6293\n",
      "epoch: 695/1000, train loss: 8.2236, val loss: 8.6366\n",
      "epoch: 696/1000, train loss: 8.2150, val loss: 8.6236\n",
      "epoch: 697/1000, train loss: 8.2209, val loss: 8.6344\n",
      "epoch: 698/1000, train loss: 8.2238, val loss: 8.6302\n",
      "epoch: 699/1000, train loss: 8.2173, val loss: 8.6356\n",
      "epoch: 700/1000, train loss: 8.2121, val loss: 8.6256\n",
      "epoch: 701/1000, train loss: 8.2080, val loss: 8.6080\n",
      "epoch: 702/1000, train loss: 8.2138, val loss: 8.6190\n",
      "epoch: 703/1000, train loss: 8.2047, val loss: 8.6115\n",
      "epoch: 704/1000, train loss: 8.2142, val loss: 8.6122\n",
      "epoch: 705/1000, train loss: 8.2168, val loss: 8.6151\n",
      "epoch: 706/1000, train loss: 8.2031, val loss: 8.6071\n",
      "epoch: 707/1000, train loss: 8.2052, val loss: 8.6072\n",
      "epoch: 708/1000, train loss: 8.2109, val loss: 8.6101\n",
      "epoch: 709/1000, train loss: 8.2079, val loss: 8.6131\n",
      "epoch: 710/1000, train loss: 8.2064, val loss: 8.6115\n",
      "epoch: 711/1000, train loss: 8.1975, val loss: 8.6039\n",
      "epoch: 712/1000, train loss: 8.2056, val loss: 8.6132\n",
      "epoch: 713/1000, train loss: 8.2019, val loss: 8.6204\n",
      "epoch: 714/1000, train loss: 8.2032, val loss: 8.6148\n",
      "epoch: 715/1000, train loss: 8.1986, val loss: 8.5977\n",
      "epoch: 716/1000, train loss: 8.2083, val loss: 8.6155\n",
      "epoch: 717/1000, train loss: 8.1982, val loss: 8.6064\n",
      "epoch: 718/1000, train loss: 8.1941, val loss: 8.6024\n",
      "epoch: 719/1000, train loss: 8.1934, val loss: 8.6090\n",
      "epoch: 720/1000, train loss: 8.1965, val loss: 8.5979\n",
      "epoch: 721/1000, train loss: 8.1911, val loss: 8.6104\n",
      "epoch: 722/1000, train loss: 8.1945, val loss: 8.6067\n",
      "epoch: 723/1000, train loss: 8.1891, val loss: 8.6048\n",
      "epoch: 724/1000, train loss: 8.1972, val loss: 8.6046\n",
      "epoch: 725/1000, train loss: 8.1897, val loss: 8.6006\n",
      "epoch: 726/1000, train loss: 8.1943, val loss: 8.6012\n",
      "epoch: 727/1000, train loss: 8.1915, val loss: 8.6115\n",
      "epoch: 728/1000, train loss: 8.1922, val loss: 8.6024\n",
      "epoch: 729/1000, train loss: 8.1849, val loss: 8.5962\n",
      "epoch: 730/1000, train loss: 8.1909, val loss: 8.6054\n",
      "epoch: 731/1000, train loss: 8.1832, val loss: 8.5987\n",
      "epoch: 732/1000, train loss: 8.1915, val loss: 8.6060\n",
      "epoch: 733/1000, train loss: 8.1858, val loss: 8.5910\n",
      "epoch: 734/1000, train loss: 8.1929, val loss: 8.5902\n",
      "epoch: 735/1000, train loss: 8.1858, val loss: 8.5791\n",
      "epoch: 736/1000, train loss: 8.1823, val loss: 8.5945\n",
      "epoch: 737/1000, train loss: 8.1769, val loss: 8.5871\n",
      "epoch: 738/1000, train loss: 8.1812, val loss: 8.5883\n",
      "epoch: 739/1000, train loss: 8.1782, val loss: 8.5776\n",
      "epoch: 740/1000, train loss: 8.1822, val loss: 8.5832\n",
      "epoch: 741/1000, train loss: 8.1776, val loss: 8.5738\n",
      "epoch: 742/1000, train loss: 8.1852, val loss: 8.5795\n",
      "epoch: 743/1000, train loss: 8.1768, val loss: 8.5801\n",
      "epoch: 744/1000, train loss: 8.1729, val loss: 8.5804\n",
      "epoch: 745/1000, train loss: 8.1762, val loss: 8.5785\n",
      "epoch: 746/1000, train loss: 8.1751, val loss: 8.5702\n",
      "epoch: 747/1000, train loss: 8.1712, val loss: 8.5683\n",
      "epoch: 748/1000, train loss: 8.1740, val loss: 8.5865\n",
      "epoch: 749/1000, train loss: 8.1698, val loss: 8.5655\n",
      "epoch: 750/1000, train loss: 8.1758, val loss: 8.5797\n",
      "epoch: 751/1000, train loss: 8.1719, val loss: 8.5686\n",
      "epoch: 752/1000, train loss: 8.1682, val loss: 8.5569\n",
      "epoch: 753/1000, train loss: 8.1725, val loss: 8.5790\n",
      "epoch: 754/1000, train loss: 8.1686, val loss: 8.5749\n",
      "epoch: 755/1000, train loss: 8.1719, val loss: 8.5603\n",
      "epoch: 756/1000, train loss: 8.1719, val loss: 8.5520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 757/1000, train loss: 8.1716, val loss: 8.5645\n",
      "epoch: 758/1000, train loss: 8.1635, val loss: 8.5511\n",
      "epoch: 759/1000, train loss: 8.1703, val loss: 8.5573\n",
      "epoch: 760/1000, train loss: 8.1658, val loss: 8.5479\n",
      "epoch: 761/1000, train loss: 8.1662, val loss: 8.5473\n",
      "epoch: 762/1000, train loss: 8.1738, val loss: 8.5483\n",
      "epoch: 763/1000, train loss: 8.1667, val loss: 8.5549\n",
      "epoch: 764/1000, train loss: 8.1754, val loss: 8.5543\n",
      "epoch: 765/1000, train loss: 8.1686, val loss: 8.5705\n",
      "epoch: 766/1000, train loss: 8.1814, val loss: 8.5689\n",
      "epoch: 767/1000, train loss: 8.1884, val loss: 8.6172\n",
      "epoch: 768/1000, train loss: 8.3962, val loss: 8.5966\n",
      "epoch: 769/1000, train loss: 8.3904, val loss: 8.7468\n",
      "epoch: 770/1000, train loss: 8.3326, val loss: 8.9828\n",
      "epoch: 771/1000, train loss: 8.2498, val loss: 8.9977\n",
      "epoch: 772/1000, train loss: 8.2331, val loss: 8.9996\n",
      "epoch: 773/1000, train loss: 8.2128, val loss: 8.9790\n",
      "epoch: 774/1000, train loss: 8.2175, val loss: 9.0183\n",
      "epoch: 775/1000, train loss: 8.2047, val loss: 9.0019\n",
      "epoch: 776/1000, train loss: 8.2129, val loss: 9.0037\n",
      "epoch: 777/1000, train loss: 8.2019, val loss: 9.0068\n",
      "epoch: 778/1000, train loss: 8.1946, val loss: 8.9682\n",
      "epoch: 779/1000, train loss: 8.2047, val loss: 9.0387\n",
      "epoch: 780/1000, train loss: 8.1914, val loss: 9.0197\n",
      "epoch: 781/1000, train loss: 8.1943, val loss: 8.9606\n",
      "epoch: 782/1000, train loss: 8.1889, val loss: 8.9771\n",
      "epoch: 783/1000, train loss: 8.1918, val loss: 8.9987\n",
      "epoch: 784/1000, train loss: 8.1826, val loss: 8.8974\n",
      "epoch: 785/1000, train loss: 8.1941, val loss: 8.9371\n",
      "epoch: 786/1000, train loss: 8.1875, val loss: 8.9362\n",
      "epoch: 787/1000, train loss: 8.1793, val loss: 8.9302\n",
      "epoch: 788/1000, train loss: 8.1800, val loss: 8.9160\n",
      "epoch: 789/1000, train loss: 8.1831, val loss: 8.9039\n",
      "epoch: 790/1000, train loss: 8.1809, val loss: 8.9346\n",
      "epoch: 791/1000, train loss: 8.1824, val loss: 8.9014\n",
      "epoch: 792/1000, train loss: 8.1838, val loss: 8.9408\n",
      "epoch: 793/1000, train loss: 8.1821, val loss: 8.9574\n",
      "epoch: 794/1000, train loss: 8.1689, val loss: 8.9463\n",
      "epoch: 795/1000, train loss: 8.1771, val loss: 8.9167\n",
      "epoch: 796/1000, train loss: 8.1735, val loss: 8.9135\n",
      "epoch: 797/1000, train loss: 8.1723, val loss: 8.9288\n",
      "epoch: 798/1000, train loss: 8.1636, val loss: 8.9260\n",
      "epoch: 799/1000, train loss: 8.1646, val loss: 8.8864\n",
      "epoch: 800/1000, train loss: 8.1650, val loss: 8.8669\n",
      "epoch: 801/1000, train loss: 8.1658, val loss: 8.8793\n",
      "epoch: 802/1000, train loss: 8.1625, val loss: 8.8942\n",
      "epoch: 803/1000, train loss: 8.1709, val loss: 8.8832\n",
      "epoch: 804/1000, train loss: 8.1587, val loss: 8.8771\n",
      "epoch: 805/1000, train loss: 8.1645, val loss: 8.8985\n",
      "epoch: 806/1000, train loss: 8.1620, val loss: 8.8931\n",
      "epoch: 807/1000, train loss: 8.1574, val loss: 8.8677\n",
      "epoch: 808/1000, train loss: 8.1595, val loss: 8.8630\n",
      "epoch: 809/1000, train loss: 8.1508, val loss: 8.8760\n",
      "epoch: 810/1000, train loss: 8.1515, val loss: 8.8823\n",
      "epoch: 811/1000, train loss: 8.1570, val loss: 8.8604\n",
      "epoch: 812/1000, train loss: 8.1483, val loss: 8.8480\n",
      "epoch: 813/1000, train loss: 8.1544, val loss: 8.8452\n",
      "epoch: 814/1000, train loss: 8.1470, val loss: 8.8682\n",
      "epoch: 815/1000, train loss: 8.1442, val loss: 8.8384\n",
      "epoch: 816/1000, train loss: 8.1503, val loss: 8.8189\n",
      "epoch: 817/1000, train loss: 8.1571, val loss: 8.8670\n",
      "epoch: 818/1000, train loss: 8.1454, val loss: 8.8453\n",
      "epoch: 819/1000, train loss: 8.1362, val loss: 8.8337\n",
      "epoch: 820/1000, train loss: 8.1442, val loss: 8.8296\n",
      "epoch: 821/1000, train loss: 8.1396, val loss: 8.7882\n",
      "epoch: 822/1000, train loss: 8.1429, val loss: 8.8267\n",
      "epoch: 823/1000, train loss: 8.1531, val loss: 8.8270\n",
      "epoch: 824/1000, train loss: 8.1486, val loss: 8.8678\n",
      "epoch: 825/1000, train loss: 8.1413, val loss: 8.8379\n",
      "epoch: 826/1000, train loss: 8.1388, val loss: 8.8232\n",
      "epoch: 827/1000, train loss: 8.1412, val loss: 8.8501\n",
      "epoch: 828/1000, train loss: 8.1274, val loss: 8.8446\n",
      "epoch: 829/1000, train loss: 8.1393, val loss: 8.8222\n",
      "epoch: 830/1000, train loss: 8.1369, val loss: 8.8544\n",
      "epoch: 831/1000, train loss: 8.1373, val loss: 8.8519\n",
      "epoch: 832/1000, train loss: 8.1352, val loss: 8.8367\n",
      "epoch: 833/1000, train loss: 8.1374, val loss: 8.8376\n",
      "epoch: 834/1000, train loss: 8.1373, val loss: 8.8817\n",
      "epoch: 835/1000, train loss: 8.1353, val loss: 8.8618\n",
      "epoch: 836/1000, train loss: 8.1262, val loss: 8.8335\n",
      "epoch: 837/1000, train loss: 8.1238, val loss: 8.8474\n",
      "epoch: 838/1000, train loss: 8.1257, val loss: 8.8251\n",
      "epoch: 839/1000, train loss: 8.1225, val loss: 8.8314\n",
      "epoch: 840/1000, train loss: 8.1218, val loss: 8.8391\n",
      "epoch: 841/1000, train loss: 8.1236, val loss: 8.8127\n",
      "epoch: 842/1000, train loss: 8.1127, val loss: 8.8527\n",
      "epoch: 843/1000, train loss: 8.1179, val loss: 8.8146\n",
      "epoch: 844/1000, train loss: 8.1131, val loss: 8.7822\n",
      "epoch: 845/1000, train loss: 8.1209, val loss: 8.8151\n",
      "epoch: 846/1000, train loss: 8.1145, val loss: 8.8104\n",
      "epoch: 847/1000, train loss: 8.1132, val loss: 8.7967\n",
      "epoch: 848/1000, train loss: 8.1147, val loss: 8.7769\n",
      "epoch: 849/1000, train loss: 8.1195, val loss: 8.7764\n",
      "epoch: 850/1000, train loss: 8.1142, val loss: 8.8127\n",
      "epoch: 851/1000, train loss: 8.1119, val loss: 8.8020\n",
      "epoch: 852/1000, train loss: 8.1184, val loss: 8.7854\n",
      "epoch: 853/1000, train loss: 8.1109, val loss: 8.8091\n",
      "epoch: 854/1000, train loss: 8.1146, val loss: 8.8209\n",
      "epoch: 855/1000, train loss: 8.1093, val loss: 8.7854\n",
      "epoch: 856/1000, train loss: 8.1105, val loss: 8.7715\n",
      "epoch: 857/1000, train loss: 8.1090, val loss: 8.7976\n",
      "epoch: 858/1000, train loss: 8.1062, val loss: 8.8113\n",
      "epoch: 859/1000, train loss: 8.1059, val loss: 8.7998\n",
      "epoch: 860/1000, train loss: 8.1076, val loss: 8.8219\n",
      "epoch: 861/1000, train loss: 8.1036, val loss: 8.8053\n",
      "epoch: 862/1000, train loss: 8.1036, val loss: 8.7977\n",
      "epoch: 863/1000, train loss: 8.1040, val loss: 8.8094\n",
      "epoch: 864/1000, train loss: 8.1065, val loss: 8.8224\n",
      "epoch: 865/1000, train loss: 8.1072, val loss: 8.7565\n",
      "epoch: 866/1000, train loss: 8.0957, val loss: 8.7926\n",
      "epoch: 867/1000, train loss: 8.0992, val loss: 8.7719\n",
      "epoch: 868/1000, train loss: 8.0920, val loss: 8.7802\n",
      "epoch: 869/1000, train loss: 8.0905, val loss: 8.7798\n",
      "epoch: 870/1000, train loss: 8.0927, val loss: 8.8205\n",
      "epoch: 871/1000, train loss: 8.0952, val loss: 8.7742\n",
      "epoch: 872/1000, train loss: 8.0976, val loss: 8.7680\n",
      "epoch: 873/1000, train loss: 8.0956, val loss: 8.7523\n",
      "epoch: 874/1000, train loss: 8.0955, val loss: 8.7773\n",
      "epoch: 875/1000, train loss: 8.0959, val loss: 8.7476\n",
      "epoch: 876/1000, train loss: 8.0929, val loss: 8.7484\n",
      "epoch: 877/1000, train loss: 8.0952, val loss: 8.7768\n",
      "epoch: 878/1000, train loss: 8.1031, val loss: 8.7787\n",
      "epoch: 879/1000, train loss: 8.0992, val loss: 8.7898\n",
      "epoch: 880/1000, train loss: 8.1034, val loss: 8.7786\n",
      "epoch: 881/1000, train loss: 8.1023, val loss: 8.7725\n",
      "epoch: 882/1000, train loss: 8.0970, val loss: 8.8054\n",
      "epoch: 883/1000, train loss: 8.0975, val loss: 8.7964\n",
      "epoch: 884/1000, train loss: 8.0937, val loss: 8.7727\n",
      "epoch: 885/1000, train loss: 8.0975, val loss: 8.7685\n",
      "epoch: 886/1000, train loss: 8.0988, val loss: 8.7922\n",
      "epoch: 887/1000, train loss: 8.1008, val loss: 8.7644\n",
      "epoch: 888/1000, train loss: 8.1026, val loss: 8.7398\n",
      "epoch: 889/1000, train loss: 8.0923, val loss: 8.7437\n",
      "epoch: 890/1000, train loss: 8.0946, val loss: 8.7290\n",
      "epoch: 891/1000, train loss: 8.0956, val loss: 8.7498\n",
      "epoch: 892/1000, train loss: 8.0987, val loss: 8.7444\n",
      "epoch: 893/1000, train loss: 8.0920, val loss: 8.7174\n",
      "epoch: 894/1000, train loss: 8.0904, val loss: 8.7047\n",
      "epoch: 895/1000, train loss: 8.0894, val loss: 8.6837\n",
      "epoch: 896/1000, train loss: 8.0799, val loss: 8.6888\n",
      "epoch: 897/1000, train loss: 8.0846, val loss: 8.7244\n",
      "epoch: 898/1000, train loss: 8.0889, val loss: 8.7232\n",
      "epoch: 899/1000, train loss: 8.0870, val loss: 8.7194\n",
      "epoch: 900/1000, train loss: 8.0769, val loss: 8.7017\n",
      "epoch: 901/1000, train loss: 8.0744, val loss: 8.7004\n",
      "epoch: 902/1000, train loss: 8.0773, val loss: 8.7011\n",
      "epoch: 903/1000, train loss: 8.0706, val loss: 8.7544\n",
      "epoch: 904/1000, train loss: 8.0698, val loss: 8.7028\n",
      "epoch: 905/1000, train loss: 8.0631, val loss: 8.7350\n",
      "epoch: 906/1000, train loss: 8.0672, val loss: 8.7356\n",
      "epoch: 907/1000, train loss: 8.0612, val loss: 8.7332\n",
      "epoch: 908/1000, train loss: 8.0623, val loss: 8.7376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 909/1000, train loss: 8.0648, val loss: 8.7419\n",
      "epoch: 910/1000, train loss: 8.0609, val loss: 8.7133\n",
      "epoch: 911/1000, train loss: 8.0625, val loss: 8.6894\n",
      "epoch: 912/1000, train loss: 8.0605, val loss: 8.6897\n",
      "epoch: 913/1000, train loss: 8.0678, val loss: 8.7312\n",
      "epoch: 914/1000, train loss: 8.0627, val loss: 8.7181\n",
      "epoch: 915/1000, train loss: 8.0643, val loss: 8.7176\n",
      "epoch: 916/1000, train loss: 8.0631, val loss: 8.6848\n",
      "epoch: 917/1000, train loss: 8.0592, val loss: 8.6962\n",
      "epoch: 918/1000, train loss: 8.0605, val loss: 8.7056\n",
      "epoch: 919/1000, train loss: 8.0660, val loss: 8.7485\n",
      "epoch: 920/1000, train loss: 8.0644, val loss: 8.6920\n",
      "epoch: 921/1000, train loss: 8.0623, val loss: 8.6770\n",
      "epoch: 922/1000, train loss: 8.0628, val loss: 8.6946\n",
      "epoch: 923/1000, train loss: 8.0626, val loss: 8.7140\n",
      "epoch: 924/1000, train loss: 8.0560, val loss: 8.7016\n",
      "epoch: 925/1000, train loss: 8.0588, val loss: 8.7287\n",
      "epoch: 926/1000, train loss: 8.0633, val loss: 8.7253\n",
      "epoch: 927/1000, train loss: 8.0621, val loss: 8.7316\n",
      "epoch: 928/1000, train loss: 8.0613, val loss: 8.7351\n",
      "epoch: 929/1000, train loss: 8.0670, val loss: 8.7378\n",
      "epoch: 930/1000, train loss: 8.0645, val loss: 8.7403\n",
      "epoch: 931/1000, train loss: 8.0625, val loss: 8.7715\n",
      "epoch: 932/1000, train loss: 8.0723, val loss: 8.7600\n",
      "epoch: 933/1000, train loss: 8.0746, val loss: 8.7523\n",
      "epoch: 934/1000, train loss: 8.0799, val loss: 8.7292\n",
      "epoch: 935/1000, train loss: 8.0833, val loss: 8.7025\n",
      "epoch: 936/1000, train loss: 8.0893, val loss: 8.6451\n",
      "epoch: 937/1000, train loss: 8.0787, val loss: 8.5991\n",
      "epoch: 938/1000, train loss: 8.0700, val loss: 8.5867\n",
      "epoch: 939/1000, train loss: 8.0585, val loss: 8.5987\n",
      "epoch: 940/1000, train loss: 8.0545, val loss: 8.6319\n",
      "epoch: 941/1000, train loss: 8.0673, val loss: 8.6009\n",
      "epoch: 942/1000, train loss: 8.0614, val loss: 8.5835\n",
      "epoch: 943/1000, train loss: 8.0574, val loss: 8.6075\n",
      "epoch: 944/1000, train loss: 8.0533, val loss: 8.5980\n",
      "epoch: 945/1000, train loss: 8.0443, val loss: 8.6181\n",
      "epoch: 946/1000, train loss: 8.0364, val loss: 8.6423\n",
      "epoch: 947/1000, train loss: 8.0406, val loss: 8.6580\n",
      "epoch: 948/1000, train loss: 8.0391, val loss: 8.6618\n",
      "epoch: 949/1000, train loss: 8.0471, val loss: 8.6645\n",
      "epoch: 950/1000, train loss: 8.0419, val loss: 8.6826\n",
      "epoch: 951/1000, train loss: 8.0352, val loss: 8.6866\n",
      "epoch: 952/1000, train loss: 8.0417, val loss: 8.6859\n",
      "epoch: 953/1000, train loss: 8.0356, val loss: 8.7011\n",
      "epoch: 954/1000, train loss: 8.0422, val loss: 8.6793\n",
      "epoch: 955/1000, train loss: 8.0400, val loss: 8.7062\n",
      "epoch: 956/1000, train loss: 8.0356, val loss: 8.6933\n",
      "epoch: 957/1000, train loss: 8.0343, val loss: 8.6859\n",
      "epoch: 958/1000, train loss: 8.0333, val loss: 8.7016\n",
      "epoch: 959/1000, train loss: 8.0316, val loss: 8.6831\n",
      "epoch: 960/1000, train loss: 8.0344, val loss: 8.6868\n",
      "epoch: 961/1000, train loss: 8.0299, val loss: 8.6939\n",
      "epoch: 962/1000, train loss: 8.0313, val loss: 8.6844\n",
      "epoch: 963/1000, train loss: 8.0341, val loss: 8.7188\n",
      "epoch: 964/1000, train loss: 8.0318, val loss: 8.7051\n",
      "epoch: 965/1000, train loss: 8.0323, val loss: 8.7278\n",
      "epoch: 966/1000, train loss: 8.0318, val loss: 8.7026\n",
      "epoch: 967/1000, train loss: 8.0318, val loss: 8.7031\n",
      "epoch: 968/1000, train loss: 8.0326, val loss: 8.7135\n",
      "epoch: 969/1000, train loss: 8.0363, val loss: 8.7091\n",
      "epoch: 970/1000, train loss: 8.0384, val loss: 8.7139\n",
      "epoch: 971/1000, train loss: 8.0427, val loss: 8.7224\n",
      "epoch: 972/1000, train loss: 8.0443, val loss: 8.7305\n",
      "epoch: 973/1000, train loss: 8.0443, val loss: 8.7165\n",
      "epoch: 974/1000, train loss: 8.0529, val loss: 8.6840\n",
      "epoch: 975/1000, train loss: 8.0724, val loss: 8.5774\n",
      "epoch: 976/1000, train loss: 8.0546, val loss: 8.5483\n",
      "epoch: 977/1000, train loss: 8.0629, val loss: 8.5127\n",
      "epoch: 978/1000, train loss: 8.0456, val loss: 8.5124\n",
      "epoch: 979/1000, train loss: 8.0288, val loss: 8.5315\n",
      "epoch: 980/1000, train loss: 8.0399, val loss: 8.5310\n",
      "epoch: 981/1000, train loss: 8.0281, val loss: 8.5549\n",
      "epoch: 982/1000, train loss: 8.0414, val loss: 8.5294\n",
      "epoch: 983/1000, train loss: 8.0330, val loss: 8.5270\n",
      "epoch: 984/1000, train loss: 8.0354, val loss: 8.5298\n",
      "epoch: 985/1000, train loss: 8.0310, val loss: 8.5613\n",
      "epoch: 986/1000, train loss: 8.0311, val loss: 8.5128\n",
      "epoch: 987/1000, train loss: 8.0331, val loss: 8.5314\n",
      "epoch: 988/1000, train loss: 8.0217, val loss: 8.5620\n",
      "epoch: 989/1000, train loss: 8.0260, val loss: 8.5456\n",
      "epoch: 990/1000, train loss: 8.0190, val loss: 8.5714\n",
      "epoch: 991/1000, train loss: 8.0270, val loss: 8.5521\n",
      "epoch: 992/1000, train loss: 8.0181, val loss: 8.5714\n",
      "epoch: 993/1000, train loss: 8.0278, val loss: 8.5677\n",
      "epoch: 994/1000, train loss: 8.0227, val loss: 8.5529\n",
      "epoch: 995/1000, train loss: 8.0231, val loss: 8.5930\n",
      "epoch: 996/1000, train loss: 8.0262, val loss: 8.5766\n",
      "epoch: 997/1000, train loss: 8.0236, val loss: 8.5731\n",
      "epoch: 998/1000, train loss: 8.0173, val loss: 8.5703\n",
      "epoch: 999/1000, train loss: 8.0168, val loss: 8.5723\n",
      "epoch: 1000/1000, train loss: 8.0169, val loss: 8.5817\n"
     ]
    }
   ],
   "source": [
    "X_train = normalize.iloc[0:len(train),:]\n",
    "X_val = normalize.iloc[len(train):,:]\n",
    "\n",
    "y_train = train['resistivity']\n",
    "y_val = val['resistivity']\n",
    "\n",
    "batch_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(512)\n",
    "batch_val = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(512)\n",
    "\n",
    "model = Net(input_dim=len(X_train.T), num_dense_layers=3, num_dense_nodes=512) # You can set strcture of ANN \n",
    "\n",
    "history = model.train(num_epochs=1000, \n",
    "                      batch_train=batch_train, \n",
    "                      batch_val=batch_val, \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ef9b9",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f198f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.99, Val R2: 0.9879\n"
     ]
    }
   ],
   "source": [
    "# Load customized electrical resistivity prediction model\n",
    "model = tf.keras.models.load_model(f'./model/ANN_customized.h5', compile=False)\n",
    "\n",
    "train_r2 = r2_score(model(tf.convert_to_tensor(X_train, dtype=tf.float32)).numpy(), y_train)\n",
    "val_r2 = r2_score(model(tf.convert_to_tensor(X_val, dtype=tf.float32)).numpy(), y_val)\n",
    "\n",
    "print('Train R2: {:.2f}, Val R2: {:.4f}'.format(train_r2, val_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e86e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load compositional features and thickness of unseen alloy compositions\n",
    "file_name = 'Test_features' # Copy and input compositional feature you want to evaluate\n",
    "df = pd.read_csv(f'./csv_test/{file_name}.csv')\n",
    "\n",
    "# Normalization using scaler fitted for features used for the model training\n",
    "df_norm = df.loc[:, norm_list]\n",
    "df_norm[:] = scaler.transform(df_norm[:])\n",
    "\n",
    "# Predict resistivity of unseen alloy compositions and save in result folder\n",
    "pred = model(tf.convert_to_tensor(df_norm, dtype=tf.float32))\n",
    "pred_df = pd.DataFrame(pred.numpy(), columns=['resistivity_pred'])\n",
    "pd.concat([df, pred_df], axis=1).to_csv(f'./result/{file_name}_pred_customized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa138434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
